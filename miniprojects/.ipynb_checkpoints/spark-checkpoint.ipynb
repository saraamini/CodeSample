{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip, sys, os, re\n",
    "from pyspark import SparkContext\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Miniproject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StackOverflow is a collaboratively edited question-and-answer site originally focused on programming topics. Because of the variety of features tracked, including a variety of feedback metrics, it allows for some open-ended analysis of user behavior on the site.\n",
    "\n",
    "StackExchange (the parent organization) provides an anonymized [data dump](https://archive.org/details/stackexchange), and we'll use Spark to perform data manipulation, analysis, and machine learning on this dataset. As a side note, there's also an online data explorer which allows you to query the data interactively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may complete this project using the Python or Scala APIs. Most questions can be done locally, however in some cases you may want to use cloud services. See the appropriate lecture notebooks for information on how to use cloud services.\n",
    "\n",
    "Python example:\n",
    "\n",
    "1. Edit source code in your main.py file, classes in a separate classes.py (Class definitions need to be written in a separate file and then included at runtime.)\n",
    "1. Run locally on a chunk using eg. `$SPARK_HOME/bin/spark-submit --py-files src/classes.py src/main.py data/stats results/stats/`\n",
    "1. Run on GCP once your testing and development are done.\n",
    "\n",
    "Scala example:\n",
    "\n",
    "1. Edit source code in Main.scala\n",
    "1. Run the command `sbt package` from the root directory of the project\n",
    "1. Use spark-submit locally on a chunk: this means adding a flag like `--master local[2]` to the spark-submit command.\n",
    "1. Run on GCP once your testing and development are done.\n",
    "\n",
    "General tips:\n",
    "* SBT has some nice features, for example continuous build and test, which can greatly speed up your development.\n",
    "* Try `cat output_dir/* | sort -n -t , -k 1.2 -o sorted_output` to concatenate your output files, which will also be in part-xxxxx format.\n",
    "* You can access an interactive Spark/Scala REPL with `$SPARK_HOME/bin/spark-shell`.\n",
    "* You can access an interactive PySpark shell with `$SPARK_HOME/bin/pyspark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available on S3 (s3://dataincubator-course/spark-stack-data). There are three subfolders: allUsers, allPosts, and allVotes which contain chunked and gzipped xml with the following format:\n",
    "\n",
    "```\n",
    "<row Body=\"&lt;p&gt;I always validate my web pages, and I recommend you do the same BUT many large company websites DO NOT and cannot validate because the importance of the website looking exactly the same on all systems requires rules to be broken. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, valid websites help your page look good even on odd configurations (like cell phones) so you should always at least try to make it validate.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2008-10-12T20:26:29.397\" Id=\"195995\" LastActivityDate=\"2008-10-12T20:26:29.397\" OwnerDisplayName=\"Eric Wendelin\" OwnerUserId=\"25066\" ParentId=\"195973\" PostTypeId=\"2\" Score=\"0\" />\n",
    "```\n",
    "\n",
    "Data from the much smaller stats.stackexchange.com is available in the same format on S3 (s3://dataincubator-course/spark-stats-data). This site, Cross-Validated, will be used below in some instances to avoid working with the full dataset for every question.\n",
    "\n",
    "The full schema is available as a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dataincubator-course/spark-stats-data/stack_exchange_schema.txt to ./stack_exchange_schema.txt\r\n"
     ]
    }
   ],
   "source": [
    "#!aws s3 cp s3://dataincubator-course/spark-stats-data/stack_exchange_schema.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either get the data by running the appropriate S3 commands in the terminal, or by running this block for the smaller stats data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!mkdir -p spark-stats-data\n",
    "#!aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data\n",
    "#!aws s3 sync --exclude '*' --include 'posts*zip' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the much larger full data set (be warned, this can take 20 or more minutes, so you may want to run it in the terminal to avoid locking up the notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!mkdir -p spark-stack-data\n",
    "#!aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stack-data/ ./spark-stack-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input and parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows are split across multiple lines; these can be discarded. Malformatted XML can also be ignored. It is enough to simply skip problematic rows, the loss of data will not significantly impact our results on this large data sets.\n",
    "\n",
    "You will need to handle xml parsing yourself using the \\ selector in Scala or something like lxml.etree in Python. *Warning*: The built-in xml.etree.ElementTree behaves differently and the results don't correspond perfectly with the Scala equivalent.\n",
    "\n",
    "To make your code more flexible, it's also recommended to incorporate command-line arguments that specify the location of the input data and where output should be written.\n",
    "\n",
    "The goal should be to have a parsing function that can be applied to the input data to access any XML element desired. It is suggested to use a class structure so that you can create RDDs of Posts, Votes, Users, etc.\n",
    "\n",
    "``` scala\n",
    "// Command line arguments in Scala\n",
    "\n",
    "object Main {\n",
    " def main(args: Array[String]) {\n",
    "   val inputDir = args(0)\n",
    "   val outputDir = args(1)\n",
    "   ...\n",
    "```\n",
    "\n",
    "``` python\n",
    "\n",
    "# Command line arguments using sysv or argparse in Python\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(ARGS.input_dir, ARGS.output_dir)\n",
    "```\n",
    "\n",
    "Dates are parsed by default using the Long data type and unix time (epoch time). In Java/Scala, a given timestamp represents the number of milliseconds since 1970-01-01T00:00:00Z. Also be wary of integer overflow when dealing with Longs. For example, these two are not equal:\n",
    "\n",
    "`val year: Long = 365 * 24 * 60 * 60 * 1000`\n",
    "\n",
    "`val year: Long = 365 * 24 * 60 * 60 * 1000L`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bad_xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "print sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple question to test your parsing code. Create an RDD of Post objects where each Post is a valid row of XML from the Cross-Validated (stats.stackexchange.com) allPosts dataset.\n",
    "\n",
    "We are going to take several shortcuts to speed up and simplify our computations.  First, your parsing function to only attempt to parse rows that start with `  <row` as these denote actual data entries. This should be done in Spark as the data is being read in from disk, without any pre-Spark processing. \n",
    "\n",
    "Return the total number XML rows that started with ` <row` that were subsequently **rejected** during your processing.  Note that the text is unicode, and contains non-ascii characters.  You may need to re-encode to utf-8 (depending on your xml parser)\n",
    "\n",
    "Note that this cleaned dataset will be used for all subsequent questions.\n",
    "\n",
    "*Question*: Can you figure out what filters you need to put in place to avoid throwing parsing errors entirely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Has_row(line):\n",
    "    return len( re.compile(u'<row ').findall(line.strip()) ) > 0\n",
    "\n",
    "def Parser(line):\n",
    "    return len( re.compile(u'<row.*?>').findall(line.strip()) ) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines with posts:\t 108741\n",
      "readable lines:\t\t 109522\n",
      "unreadable posts:\t 781\n"
     ]
    }
   ],
   "source": [
    "my_input_dir = 'spark-stats-data/allPosts/'\n",
    "All_lines = sc.textFile(localpath(my_input_dir)).map(Has_row).filter(lambda x: x is True).count()\n",
    "Fine_lines = sc.textFile(localpath(my_input_dir)).map(Parser).filter(lambda x: x is True).count()\n",
    "Missed_ones = All_lines - Fine_lines\n",
    "\n",
    "print(\"lines with posts:\\t {0}\".format(Fine_lines) )\n",
    "print(\"readable lines:\\t\\t {0}\".format(All_lines) )\n",
    "print(\"unreadable posts:\\t {0}\".format(Missed_ones) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def bad_xml():\n",
    "    return Missed_ones\n",
    "\n",
    "grader.score(question_name='spark__bad_xml', func=bad_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upvote_percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each post on StackExchange can be upvoted, downvoted, and favorited. One \"sanity check\" we can do is to look at the ratio of upvotes to downvotes (referred to as \"UpMod\" and \"DownMod\" in the schema) as a function of how many times the post has been favorited.\n",
    "\n",
    "You might hypothesize, for example, that posts with more favorites should have a higher upvote/downvote ratio.\n",
    "\n",
    "Instead of looking at individual posts, we'll aggregate across number of favorites by using the post's number of favorites as our key. Since we're computing ratios, bundling together all posts with the same number of favorites effectively averages over them.  Calculate the average percentage of upvotes *(upvotes / (upvotes + downvotes))* for the smallest 50 **keys**.\n",
    "\n",
    "Do the analysis on the smaller Cross-Validated dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total upvotes: 313,819\n",
    "* Total downvotes: 13,019\n",
    "* Mean of first 50 keys (averaging the keys themselves): 24.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#u'(^<row(.*?)\\/>)'\n",
    "def Vote(line):\n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        PostId = int(root.get('PostId'))\n",
    "        Up, Down, Fav = 0, 0, 0\n",
    "        var = int(root.get('VoteTypeId'))\n",
    "        if var == 2: Up += 1\n",
    "        elif var == 3: Down += 1\n",
    "        elif var == 5: Fav += 1\n",
    "        return (PostId, Up, Down, Fav)\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = 'spark-stats-data/allVotes/'\n",
    "sc.textFile(localpath(my_input_dir)).map(Vote).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = 'spark-stats-data/allVotes/'\n",
    "\n",
    "top_50 = sc.textFile(localpath(my_input_dir)).map(Vote).filter(lambda x: x!=[])\\\n",
    "        .map(lambda x: (x[0], (x[1], x[2], x[3])) )\\\n",
    "        .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2]) )\\\n",
    "        .map(lambda x: (x[1][2], (x[1][0], x[1][1])) )\\\n",
    "        .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]) )\\\n",
    "        .map(lambda x: (  x[0], x[1][0]/float(x[1][0]+x[1][1])  )  )\\\n",
    "        .sortByKey(ascending=True).take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_50[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upvote_percentage():\n",
    "    return top_50 #[(20, 0.9952153110047847)] * 50\n",
    "\n",
    "grader.score(question_name='spark__upvote_percentage', func=upvote_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answer_percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the correlation between a user's reputation and the kind of posts they make. For the 99 users with the highest reputation, single out posts which are either questions or answers and look at the percentage of these posts that are answers: *(answers / (answers + questions))*. \n",
    "\n",
    "Return a tuple of their **user ID** and this fraction.\n",
    "\n",
    "You should also return (-1, fraction) to represent the case where you average over all users (so you will return 100 entries total).\n",
    "\n",
    "Again, you only need to run this on the statistics overflow set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total questions: 52,060\n",
    "* Total answers: 55,304\n",
    "* Top 99 users' average reputation: 11893.464646464647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def User_Rep(line):\n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)/>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        Id = int(root.get('Id'))\n",
    "        Reputation = int(root.get('Reputation'))\n",
    "        return (Id, Reputation)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def User_Posts(line):\n",
    "    \n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)/>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        UserId = int(root.attrib['OwnerUserId'])\n",
    "            \n",
    "        questions, answers = 0, 0\n",
    "            \n",
    "        var = int(root.get('PostTypeId')) \n",
    "        if var == 1: questions += 1\n",
    "        elif var == 2: answers += 1\n",
    "            \n",
    "        return (UserId, (questions, answers))\n",
    "\n",
    "    except:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_Posts = 'spark-stats-data/allPosts/'\n",
    "my_input_Users = 'spark-stats-data/allUsers/'\n",
    "\n",
    "posts = sc.textFile(localpath(my_input_Posts)).map(User_Posts).filter(lambda x: x!=[])\n",
    "users = sc.textFile(localpath(my_input_Users)).map(User_Rep).filter(lambda x: x!=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####CHECK POINT: Top 99 users' average reputation: 11893.464646464647 CHECK :)\n",
    "top99users = users\\\n",
    "    .map(lambda x: (x[1],(1))) \\\n",
    "    .sortByKey(ascending=False) \\\n",
    "    .take(99)\n",
    "sum([i[0] for i in top99users])/float(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####CHECK POINT: Total questions: 52,060\n",
    "####CHECK POINT: Total answers: 55,304\n",
    "totans = posts.filter(lambda x: x[1][1]!=0)\\\n",
    "    .map(lambda x: (1,(x[1][1]))).collect()\n",
    "\n",
    "# I GET TOTAL ANS = 54557 which is != 55,304\n",
    "\n",
    "totques = posts.filter(lambda x: x[1][0]!=0)\\\n",
    "    .map(lambda x: (1,(x[1][0]))).collect()\n",
    "    \n",
    "# I GET TOTAL QUES = 51222 which is != 52,060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55304 52060\n"
     ]
    }
   ],
   "source": [
    "print len(totans),len(totques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, (1, 0)), (24, (1, 0)), (18, (1, 0)), (23, (1, 0)), (23, (0, 1))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, 1), (2, 101), (3, 101), (4, 101), (5, 6962)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40963, (11, (1, 0))),\n",
       " (24583, (3, (1, 0))),\n",
       " (16393, (300, (1, 0))),\n",
       " (16393, (300, (1, 0)))]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.join(posts).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40963, ((1, 0), 11)),\n",
       " (24583, ((1, 0), 3)),\n",
       " (16393, ((1, 0), 300)),\n",
       " (16393, ((1, 0), 300))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.join(users).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = users.join(posts)\\\n",
    "        .map(lambda x: (x[0], (x[1][0], x[1][1][0], x[1][1][1]))) \\\n",
    "        .reduceByKey(lambda x,y: (x[0], x[1]+y[1], x[2]+y[2]) ) \\\n",
    "        .filter(lambda x: x[1][1]+x[1][2]!=0 ) \\\n",
    "        .map(lambda x: (x[1][0], (x[0],x[1][2]/float(x[1][1]+x[1][2]))) ) \\\n",
    "        .sortByKey(ascending=False) \\\n",
    "        .take(99)\n",
    "        \n",
    "         #(id,(rep,(q,a)))\n",
    "         #compress id! (id,(rep,q,a))\n",
    "         #make sure u don't devide by 0\n",
    "         #(rep, (id,a/(q+a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(69634, (404, 0.7777777777777778)),\n",
       " (68914, (99, 0.2647058823529412)),\n",
       " (68514, (281, 0.4666666666666667)),\n",
       " (68133, (104, 0.2755102040816326)),\n",
       " (67799, (443, 0.9166666666666666))]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_all_users = posts.map(lambda x: (1, (x[1][0], x[1][1])) )\\\n",
    "        .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]) )\\\n",
    "        .map(lambda x: (x[1][1]/float(x[1][0]+x[1][1])) )\\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q3 = [i[1] for i in table]\n",
    "q3.append((-1,avg_all_users[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(919, 0.996694214876033),\n",
       " (805, 0.9959749552772809),\n",
       " (686, 0.9803049555273189)]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def answer_percentage():\n",
    "    return q3 #[(7071, 0.9107142857142857)] * 100\n",
    "\n",
    "grader.score(question_name='spark__answer_percentage', func=answer_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the total number of posts made on the site as a metric for tenure, we can look at the differences between \"younger\" and \"older\" users. You can imagine there might be many interesting features - for now just return the top 100 post counts among all users (of all types of posts) and the average reputation for every user who has that count.\n",
    "\n",
    "In other words, aggregate the cases where multiple users have the same post count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean of top 100 post counts: 281.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Posts_number(line):    \n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        if 'OwnerUserId' in root.attrib:\n",
    "            OwnerUserId = int(root.attrib['OwnerUserId'])\n",
    "        else:\n",
    "            OwnerUserId = -2\n",
    "                \n",
    "        return (OwnerUserId, 1)\n",
    "        \n",
    "    except:\n",
    "        return []\n",
    "   \n",
    "    \n",
    "def User_rep(line):\n",
    "    \n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        Id, Reputation = int(root.attrib['Id']), int(root.attrib['Reputation'])\n",
    "        return (Id, Reputation)\n",
    "            \n",
    "    except:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_Posts = 'spark-stats-data/allPosts/'\n",
    "my_input_Users = 'spark-stats-data/allUsers/'\n",
    "\n",
    "posts_num = sc.textFile(localpath(my_input_Posts))\\\n",
    "        .map(Posts_number).filter(lambda x: x!=[])\\\n",
    "        .reduceByKey(lambda x,y: x+y)\n",
    "#posts_num = (user id,number of posts for each user)\n",
    "users = sc.textFile(localpath(my_input_Users)).map(User_Rep).filter(lambda x: x!=[])\n",
    "# Users = (user id , rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40963, (11, 1)), (24583, (3, 1)), (16393, (300, 1)), (16393, (300, 1))]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.join(posts_num).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40963, (1, 11)), (24583, (1, 3)), (16393, (14, 300)), (13, (7, 947))]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_num.join(users).take(4)\n",
    "# (userid, (post number,rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = posts_num.join(users)\\\n",
    "        .map(lambda x: (x[1][0], (x[1][1],1)))\\\n",
    "        .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\\\n",
    "        .map(lambda x : (x[0],(x[1][0]/x[1][1])))\\\n",
    "        .sortByKey(ascending=False)\\\n",
    "        .take(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2325, 92624), (1663, 47334), (1287, 100976), (1018, 46907), (965, 23102)]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def post_counts():\n",
    "    return table #[(118, 3736.5)] * 100\n",
    "\n",
    "grader.score(question_name='spark__post_counts', func=post_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long do you have to wait to get your question answered? Look at the set of ACCEPTED answers which are posted less than three hours after question creation. What is the average number of these \"quick answers\" as a function of the hour of day the question was asked? You should normalize by how many total accepted answers are garnered by questions posted in a given hour, just like we're counting how many quick accepted answers are garnered by questions posted in a given hour, eg. (quick accepted answers when question hour is 15 / total accepted answers when question hour is 15).\n",
    "\n",
    "Return a list, whose ith element correspond to ith hour (e.g. 0 -> midnight, 1 -> 1:00, etc.)\n",
    "\n",
    "*Note*: When using Scala's SimpleDateFormat class, it's important to account for your machine's local time zone. Our policy will be to use GMT: hourFormat.setTimeZone(TimeZone.getTimeZone(\"GMT\"))\n",
    "\n",
    "*Consider*: What biases are present in our result that we don't account for? How should we handle this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total quick accepted answers: 8,468\n",
    "* Total accepted answers: 17,096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Posts_Q(line): \n",
    "        \n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        Id = int(root.attrib['Id'])\n",
    "        CreationDate = datetime.strptime(root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f').replace(microsecond=0)\n",
    "        PostTypeId = int(root.attrib['PostTypeId'])\n",
    "\n",
    "        if PostTypeId == 1:\n",
    "            AcceptedAnswerId = int(root.attrib['AcceptedAnswerId'])\n",
    "            return (Id, AcceptedAnswerId, CreationDate)\n",
    "\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "def Posts_A(line): \n",
    "        \n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        Id = int(root.attrib['Id'])\n",
    "        CreationDate = datetime.strptime(root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f').replace(microsecond=0)\n",
    "        PostTypeId = int(root.attrib['PostTypeId'])\n",
    "\n",
    "        if PostTypeId == 2:\n",
    "            return (Id, CreationDate)\n",
    "\n",
    "    except:\n",
    "        return []    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_Posts = 'spark-stats-data/allPosts/'\n",
    "\n",
    "Question = sc.textFile(localpath(my_input_Posts))\\\n",
    "    .map(Posts_Q)\\\n",
    "    .filter(lambda x: x!=[])\\\n",
    "    .filter(lambda x: x is not None)\n",
    "\n",
    "#FORMAT QUESTIONS: (1, 15, datetime.datetime(2010, 7, 19, 19, 12, 12), 'Q')\n",
    "#(Id, AcceptedAnswerId, CreationDate)\n",
    "\n",
    "Answer = sc.textFile(localpath(my_input_Posts))\\\n",
    "    .map(Posts_A)\\\n",
    "    .filter(lambda x: x!=[])\\\n",
    "    .filter(lambda x: x is not None)\n",
    "#FORMAT ANSWER (Id, CreationDate)\n",
    "    \n",
    "Answered_Questions = Question.map(lambda x: (x[1], x[2]))\\\n",
    "    .join( Answer.map(lambda x: (x[0],x[1])))    \n",
    "# Answered_Questions = (AcceptedAnswerId, (questions CreationDate,answers CreationDate))\n",
    "    \n",
    "Quick_answered = Answered_Questions.filter(lambda x: (x[1][1]-x[1][0]<timedelta(0, 10800)) )\n",
    "# SAME FORMAT AS ANSWERED_QUESTIONS\n",
    "\n",
    "Number_of_Answered_Questions = Answered_Questions.map(lambda x: (x[1][0].hour ,1) )\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .sortByKey()\n",
    "Number_of_Quick_answered = Quick_answered.map(lambda x: (x[1][0].hour ,1) )\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .sortByKey()  \n",
    "    \n",
    "wait_time = Number_of_Quick_answered.map(lambda x: (x[0], x[1]))\\\n",
    "    .join( Number_of_Answered_Questions.map(lambda x: (x[0],x[1])))\\\n",
    "    .map(lambda x: (x[1][0]/float(x[1][1]))).collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4504672897196262,\n",
       " 0.44814814814814813,\n",
       " 0.3605577689243028,\n",
       " 0.3799126637554585,\n",
       " 0.4028436018957346,\n",
       " 0.4125,\n",
       " 0.4597402597402597,\n",
       " 0.4673684210526316,\n",
       " 0.4616822429906542,\n",
       " 0.49528301886792453]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait_time[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def quick_answers():\n",
    "    return wait_time #[0.] * 24\n",
    "\n",
    "grader.score(question_name='spark__quick_answers', func=quick_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick_answers_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but on the full StackExchange dataset.\n",
    "\n",
    "No pre-parsed data is available for this question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total quick accepted answers: 3,700,224\n",
    "* Total accepted answers: 5,086,888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### VERY SAME CODE FROM LAST PART ONY CHANGE THE INPUT!\n",
    "\n",
    "my_input_Posts = 'spark-stack-data/allPosts/'\n",
    "\n",
    "Question = sc.textFile(localpath(my_input_Posts))\\\n",
    "    .map(Posts_Q)\\\n",
    "    .filter(lambda x: x!=[])\\\n",
    "    .filter(lambda x: x is not None)\n",
    "\n",
    "#FORMAT QUESTIONS: (1, 15, datetime.datetime(2010, 7, 19, 19, 12, 12), 'Q')\n",
    "#(Id, AcceptedAnswerId, CreationDate)\n",
    "\n",
    "Answer = sc.textFile(localpath(my_input_Posts))\\\n",
    "    .map(Posts_A)\\\n",
    "    .filter(lambda x: x!=[])\\\n",
    "    .filter(lambda x: x is not None)\n",
    "#FORMAT ANSWER (Id, CreationDate)\n",
    "    \n",
    "Answered_Questions = Question.map(lambda x: (x[1], x[2]))\\\n",
    "    .join( Answer.map(lambda x: (x[0],x[1])))    \n",
    "# Answered_Questions = (AcceptedAnswerId, (questions CreationDate,answers CreationDate))\n",
    "    \n",
    "Quick_answered = Answered_Questions.filter(lambda x: (x[1][1]-x[1][0]<timedelta(0, 10800)) )\n",
    "# SAME FORMAT AS ANSWERED_QUESTIONS\n",
    "\n",
    "Number_of_Answered_Questions = Answered_Questions.map(lambda x: (x[1][0].hour ,1) )\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .sortByKey()\n",
    "Number_of_Quick_answered = Quick_answered.map(lambda x: (x[1][0].hour ,1) )\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .sortByKey()  \n",
    "    \n",
    "wait_time = Number_of_Quick_answered.map(lambda x: (x[0], x[1]))\\\n",
    "    .join( Number_of_Answered_Questions.map(lambda x: (x[0],x[1])))\\\n",
    "    .map(lambda x: (x[1][0]/float(x[1][1]))).collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.690509573675678,\n",
       " 0.6960107966498631,\n",
       " 0.6996508810120343,\n",
       " 0.7043822789835892,\n",
       " 0.7101039989211401,\n",
       " 0.7178884888038708,\n",
       " 0.7242146432479951,\n",
       " 0.7270583958385681,\n",
       " 0.726148461144255,\n",
       " 0.722902538582882]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait_time[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def quick_answers_full():\n",
    "    return wait_time #[0.] * 24\n",
    "\n",
    "grader.score(question_name='spark__quick_answers_full', func=quick_answers_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify_veterans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, we'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, we'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users.\n",
    "\n",
    "*Consider*: What other parameterizations of \"activity\" could we use, and how would they differ in terms of splitting our user base?\n",
    "\n",
    "*Consider*: What other biases are still not dealt with, after using the above approach?\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of \"veterans\" vs. \"brief users\". For each group separately, average the score, views, number of answers, and number of favorites of the users' **first question**.\n",
    "\n",
    "*Consider*: What story could you tell from these numbers? How do the numbers support it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total brief users: 24,864\n",
    "* Total veteran users: 2,027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def User_Info(line):\n",
    "    \n",
    "        \n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)>)')\\\n",
    "                             .findall(line.strip())[0][0].encode('utf-8').strip())          \n",
    "        Id = int(root.attrib['Id']) \n",
    "        CreationDate = datetime.strptime(root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f').replace(microsecond=0)\n",
    "        return (Id, CreationDate)            \n",
    "\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    \n",
    "def Post_Info(line):\n",
    "    \n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)>)').findall(line.strip())[0][0].encode('utf-8').strip())\n",
    "        OwnerUserId = int(root.attrib['OwnerUserId'])\n",
    "        PostTypeId = int(root.attrib['PostTypeId'])\n",
    "        CreationDate = datetime.strptime(root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f').replace(microsecond=0)\n",
    "        \n",
    "        if 'Score' in root.attrib: \n",
    "            Score = int(root.attrib['Score'])\n",
    "        else: \n",
    "            Score = 0\n",
    "\n",
    "        if 'ViewCount' in root.attrib: \n",
    "            ViewCount = int(root.attrib['ViewCount'])\n",
    "        else: \n",
    "            ViewCount = 0\n",
    "\n",
    "        if 'AnswerCount' in root.attrib: \n",
    "            AnswerCount = int(root.attrib['AnswerCount'])\n",
    "        else: \n",
    "            AnswerCount = 0\n",
    "\n",
    "        if 'FavoriteCount' in root.attrib: \n",
    "            FavoriteCount = int(root.attrib['FavoriteCount'])\n",
    "        else: \n",
    "            FavoriteCount = 0\n",
    "        '''\n",
    "        Score = int(root.attrib['Score'])\n",
    "        ViewCount = int(root.attrib['ViewCount'])\n",
    "        AnswerCount = int(root.attrib['AnswerCount'])\n",
    "        FavoriteCount = int(root.attrib['FavoriteCount'])\n",
    "        '''\n",
    "        return (OwnerUserId, PostTypeId, CreationDate, Score, ViewCount, AnswerCount, FavoriteCount)            \n",
    "\n",
    "    except:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_Users = 'spark-stats-data/allUsers/'\n",
    "\n",
    "Usr_Info = sc.textFile(localpath(my_input_Users))\\\n",
    "    .map(User_Info)\\\n",
    "    .filter(lambda x: x != [])\n",
    "\n",
    "my_input_Posts = 'spark-stats-data/allPosts/'\n",
    "\n",
    "Pst_Info = sc.textFile(localpath(my_input_Posts))\\\n",
    "    .map(Post_Info)\\\n",
    "    .filter(lambda x: x != [])\n",
    "\n",
    "    \n",
    "User_first_Q = sc.textFile(localpath(my_input_Posts))\\\n",
    "    .map(Post_Info)\\\n",
    "    .filter(lambda x: x !=[] and x[1]==1)\\\n",
    "    .map(lambda x: (x[0], (x[1],x[2],x[3],x[4],x[5],x[6]) ))\\\n",
    "    .reduceByKey(lambda x, y: x if x[1]<y[1] else y)\n",
    "\n",
    "#all the info but for just first question: (OwnerUserId, PostTypeId, CreationDate, Score,  \n",
    "                                            #ViewCount,AnswerCount, FavoriteCount)\n",
    "\n",
    "active_users = Usr_Info.join(Pst_Info.map(lambda x: (x[0], x[2])))\\\n",
    "    .map(lambda x: (x[0], 1) if timedelta(100, 0)<x[1][1]-x[1][0]<timedelta(150, 0) else (x[0], 0) )\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .filter(lambda x: x[1]>0)\\\n",
    "    .map(lambda x: (x[0],1) )\n",
    "#After join we have (userid,(creation date,post date))\n",
    "#first filter goves us the users who have posts between 100 to 150 days after they create their id\n",
    "#reduce by key gives us how many posts a user had between day 100 to 150 after creating id\n",
    "# the filter keeps those who have posts between day 100-150 which is the defination of active user\n",
    "# finaly we end up having (active users id ,1)\n",
    "\n",
    "inactive_users = Usr_Info.join(Pst_Info.map(lambda x: (x[0], x[2]))  )\\\n",
    "    .map(lambda x: (x[0], 1) if timedelta(100, 0)<x[1][1]-x[1][0]<timedelta(150, 0) else (x[0], 0) )\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .filter(lambda x: x[1]==0)\\\n",
    "    .map(lambda x: (x[0],1) )\n",
    "#same as before but if the user doesn't have any posts between day 100-150 after creating his/her\n",
    "#account then we call them inactive users\n",
    "\n",
    "active_info = active_users.join(User_first_Q)\\\n",
    "    .map(lambda x: (1,(x[1][1][2],x[1][1][3],x[1][1][4],x[1][1][5],1)))\\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]) )\\\n",
    "    .map(lambda x: (x[1][0]/float(x[1][4]),x[1][1]/float(x[1][4]),x[1][2]/float(x[1][4]),x[1][3]/float(x[1][4])))\\\n",
    "    .collect()\n",
    "#gives out : Score, ViewCount, AnswerCount, FavoriteCount,1 (one helps us get the \n",
    "                                                        #total number of actives later)\n",
    "    #finally we get ave of Score, ViewCount, AnswerCount, FavoriteCount for active users\n",
    "\n",
    "inactive_info = inactive_users.join(User_first_Q)\\\n",
    "    .map(lambda x: (1,(x[1][1][2],x[1][1][3],x[1][1][4],x[1][1][5],1)))\\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]) )\\\n",
    "    .map(lambda x: (x[1][0]/float(x[1][4]),x[1][1]/float(x[1][4]),x[1][2]/float(x[1][4]),x[1][3]/float(x[1][4])))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active user: \t Score = 3.54\t ViewCount = 926.40\t AnswerCount = 1.30\t FavoriteCount = 1.30\n",
      "inactive user: \t Score = 2.10\t ViewCount = 553.52\t AnswerCount = 0.97\t FavoriteCount = 0.58\n"
     ]
    }
   ],
   "source": [
    "print('active user: \\t Score = {0:.2f}\\t ViewCount = {1:.2f}\\t AnswerCount = {2:.2f}\\t FavoriteCount = {3:.2f}'\\\n",
    "      .format(active_info[0][0],active_info[0][1],active_info[0][2],active_info[0][3]) )\n",
    "print('inactive user: \\t Score = {0:.2f}\\t ViewCount = {1:.2f}\\t AnswerCount = {2:.2f}\\t FavoriteCount = {3:.2f}'\\\n",
    "      .format(inactive_info[0][0],inactive_info[0][1],inactive_info[0][2],inactive_info[0][3]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def identify_veterans():\n",
    "    return {\"vet_score\": active_info[0][0],\n",
    "            \"vet_views\": active_info[0][1],\n",
    "            \"vet_answers\": active_info[0][2],\n",
    "            \"vet_favorites\": active_info[0][3],\n",
    "            \"brief_score\": inactive_info[0][0],\n",
    "            \"brief_views\": inactive_info[0][1],\n",
    "            \"brief_answers\": inactive_info[0][2],\n",
    "            \"brief_favorites\": inactive_info[0][3]\n",
    "           }\n",
    "\n",
    "grader.score(question_name='spark__identify_veterans', func=identify_veterans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify_veterans_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but on the full StackExchange dataset.\n",
    "\n",
    "No pre-parsed data is available for this question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total brief users: 1,848,628\n",
    "* Total veteran users: 288,285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_Users = 'spark-stack-data/allUsers/'\n",
    "\n",
    "Usr_Info = sc.textFile(localpath(my_input_Users))\\\n",
    "    .map(User_Info)\\\n",
    "    .filter(lambda x: x != [])\n",
    "\n",
    "my_input_Posts = 'spark-stack-data/allPosts/'\n",
    "\n",
    "Pst_Info = sc.textFile(localpath(my_input_Posts))\\\n",
    "    .map(Post_Info)\\\n",
    "    .filter(lambda x: x != [])\n",
    "\n",
    "    \n",
    "User_first_Q = sc.textFile(localpath(my_input_Posts))\\\n",
    "    .map(Post_Info)\\\n",
    "    .filter(lambda x: x !=[] and x[1]==1)\\\n",
    "    .map(lambda x: (x[0], (x[1],x[2],x[3],x[4],x[5],x[6]) ))\\\n",
    "    .reduceByKey(lambda x, y: x if x[1]<y[1] else y)\n",
    "\n",
    "#all the info but for just first question: (OwnerUserId, PostTypeId, CreationDate, Score,  \n",
    "                                            #ViewCount,AnswerCount, FavoriteCount)\n",
    "\n",
    "active_users = Usr_Info.join(Pst_Info.map(lambda x: (x[0], x[2])))\\\n",
    "    .map(lambda x: (x[0], 1) if timedelta(100, 0)<x[1][1]-x[1][0]<timedelta(150, 0) else (x[0], 0) )\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .filter(lambda x: x[1]>0)\\\n",
    "    .map(lambda x: (x[0],1) )\n",
    "#After join we have (userid,(creation date,post date))\n",
    "#first filter goves us the users who have posts between 100 to 150 days after they create their id\n",
    "#reduce by key gives us how many posts a user had between day 100 to 150 after creating id\n",
    "# the filter keeps those who have posts between day 100-150 which is the defination of active user\n",
    "# finaly we end up having (active users id ,1)\n",
    "\n",
    "inactive_users = Usr_Info.join(Pst_Info.map(lambda x: (x[0], x[2]))  )\\\n",
    "    .map(lambda x: (x[0], 1) if timedelta(100, 0)<x[1][1]-x[1][0]<timedelta(150, 0) else (x[0], 0) )\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .filter(lambda x: x[1]==0)\\\n",
    "    .map(lambda x: (x[0],1) )\n",
    "#same as before but if the user doesn't have any posts between day 100-150 after creating his/her\n",
    "#account then we call them inactive users\n",
    "\n",
    "active_info = active_users.join(User_first_Q)\\\n",
    "    .map(lambda x: (1,(x[1][1][2],x[1][1][3],x[1][1][4],x[1][1][5],1)))\\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]) )\\\n",
    "    .map(lambda x: (x[1][0]/float(x[1][4]),x[1][1]/float(x[1][4]),x[1][2]/float(x[1][4]),x[1][3]/float(x[1][4])))\\\n",
    "    .collect()\n",
    "#gives out : Score, ViewCount, AnswerCount, FavoriteCount,1 (one helps us get the \n",
    "                                                        #total number of actives later)\n",
    "    #finally we get ave of Score, ViewCount, AnswerCount, FavoriteCount for active users\n",
    "\n",
    "inactive_info = inactive_users.join(User_first_Q)\\\n",
    "    .map(lambda x: (1,(x[1][1][2],x[1][1][3],x[1][1][4],x[1][1][5],1)))\\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]) )\\\n",
    "    .map(lambda x: (x[1][0]/float(x[1][4]),x[1][1]/float(x[1][4]),x[1][2]/float(x[1][4]),x[1][3]/float(x[1][4])))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def identify_veterans_full():\n",
    "    return {\"vet_score\": active_info[0][0],\n",
    "            \"vet_views\": active_info[0][1],\n",
    "            \"vet_answers\": active_info[0][2],\n",
    "            \"vet_favorites\": active_info[0][3],\n",
    "            \"brief_score\": inactive_info[0][0],\n",
    "            \"brief_views\": inactive_info[0][1],\n",
    "            \"brief_answers\": inactive_info[0][2],\n",
    "            \"brief_favorites\": inactive_info[0][3]\n",
    "           }\n",
    "\n",
    "grader.score(question_name='spark__identify_veterans_full', func=identify_veterans_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is an alternative approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the tags of each StackExchange post as documents (this uses the full dataset). Use Spark ML's implementation of Word2Vec (this will require using DataFrames) to return a list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality of the vector space should be 100. The random seed should be 42L.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean of the top 25 cosine similarities: 0.7785175901170094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed to convert RDDs into DataFrames\n",
    "#sqlContext = SQLContext(sc)\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Posts_tag(line):\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(re.compile(u'(<row(.*?)>)')\\\n",
    "            .findall(line.strip())[0][0].encode('utf-8').strip()) \n",
    "        Tags = re.compile('\\w+').findall( root.attrib['Tags'] )\n",
    "        return (Tags)\n",
    "    except:\n",
    "        return []\n",
    " ### Apprerantly r'\\w[\\w-]+' does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_dir = 'spark-stack-data/allPosts/'\n",
    "\n",
    "my_tags = sc.textFile(localpath(my_input_dir))\\\n",
    "    .map(Posts_tag)\\\n",
    "    .filter(lambda x: x!=[])\\\n",
    "    .map(lambda x: (x,1))\\\n",
    "    .toDF(['tags', 'var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tags=[u'java', u'testing', u'selenium', u'selenium', u'webdriver', u'ui', u'automation'], var=1),\n",
       " Row(tags=[u'angularjs', u'angularjs', u'scope', u'angular', u'http', u'interceptors'], var=1),\n",
       " Row(tags=[u'embedded', u'windows', u'ce', u'bootable'], var=1),\n",
       " Row(tags=[u'iframe', u'primefaces'], var=1),\n",
       " Row(tags=[u'three', u'js'], var=1),\n",
       " Row(tags=[u'android', u'android', u'layout', u'android', u'linearlayout', u'android', u'textview'], var=1),\n",
       " Row(tags=[u'asp', u'net', u'memory', u'leaks', u'net', u'4', u'0', u'net', u'4', u'5', u'stateserver'], var=1),\n",
       " Row(tags=[u'php', u'arrays', u'associative', u'array', u'shuffle'], var=1),\n",
       " Row(tags=[u'python', u'django', u'django', u'crispy', u'forms'], var=1),\n",
       " Row(tags=[u'php', u'excel', u'import'], var=1)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tags.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec(vectorSize=100, seed=42L, inputCol='tags', outputCol='vectors', minCount=1)\n",
    "my_model = w2v.fit(my_tags)\n",
    "\n",
    "vectors = my_model.getVectors().rdd.map(lambda x: (x.word, x.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectors.collect()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_synonyms = 25\n",
    "my_synonyms = my_model.findSynonyms(\"ggplot2\", n_synonyms).select(\"word\", \"similarity\").take(n_synonyms)\n",
    "similars = []\n",
    "for i in range(n_synonyms):\n",
    "    similars.append( (my_synonyms[i][0], my_synonyms[i][1]) )    \n",
    "#similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-2d0a3f7fa1b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "sc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([i[1] for i in similars])/25  \n",
    "# just by using non Hyphenated words I got 0.79\n",
    "# By using both Hyphenated and non hyphenated words I got 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec():\n",
    "    return similars#[(\"data.frame\", 0.7900882217638416)] * 25\n",
    "\n",
    "grader.score(question_name='spark__word2vec', func=word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
