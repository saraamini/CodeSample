{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Labeling Based on Expiration Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this step, I first sort the raw data (transactions.csv) by the users' ID: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./Data/transactions.csv | sort --field-separator=',' --key=1 > sorted_trans.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step is to make labels based on expiration date. The code below, forst read all the id's as key and the info for each id as values (each key can have more than 1 value since the user can have many transactions through time.)\n",
    "Each user considered as churned in 2 cases:\n",
    "1. If the most recent transaction has been canceled; \n",
    "or\n",
    "2. If the expiration date for the most recent transaction has been past. (the current date has been defined as the latest date available in transaction date (Among all the users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Python code for churn labeling (labeling.py):\n",
    "#! /usr/bin/env python\n",
    "\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def compute_lf(subscription_map):\n",
    "  date_of_latest_transaction = datetime.strptime('20170228','%Y%m%d').date()\n",
    "  for k,records in subscription_map.iteritems():\n",
    "    # a set of transactions are considered bad if a subscription is canceled and then\n",
    "    # it's followed by more transactions. \n",
    "    bad_transactions = False\n",
    "    for record in records[:-1]:\n",
    "      if record[-1] == '1':\n",
    "        bad_transactions = True\n",
    "        break\n",
    "\n",
    "    if bad_transactions:\n",
    "      continue\n",
    "      #print 'Found bad transactions for %s' %k\n",
    "    # This is the date of the first transaction\n",
    "    # This is the expiration date of the last transaction\n",
    "    expiration_date = datetime.strptime(records[-1][7],'%Y%m%d').date()\n",
    "\n",
    "    subscription_canceled = records[-1][-1] == '1'\n",
    "    churn = '0'\n",
    "    if subscription_canceled == False:\n",
    "      # if the expiration date of the last transaction is before the date\n",
    "      # of the latest transaction in the dataset, this is considered to be a churn.\n",
    "      if expiration_date < date_of_latest_transaction:\n",
    "        subscription_canceled = True\n",
    "        #print 'This is actually a churn: %s' %k\n",
    "    if subscription_canceled:\n",
    "      churn = '1'\n",
    "    print k +  ',' + churn\n",
    "\n",
    "\n",
    "def generate_clean_data():\n",
    "  input_file = open('./sorted_trans.csv', 'r')\n",
    "  lines = input_file.readlines()\n",
    "  lines = [line.strip() for line in lines]\n",
    "  sorted_lines = sorted(lines, key= lambda line : line.split(',')[7])\n",
    "  subscription_map = {}\n",
    "\n",
    "  for line in sorted_lines:\n",
    "    transaction_id = line.split(',')[0]\n",
    "    transaction_record = line.split(',')[1:]\n",
    "    if subscription_map.has_key(transaction_id):\n",
    "      subscription_map[transaction_id].append(transaction_record)\n",
    "    else:\n",
    "       subscription_map[transaction_id] = [transaction_record]\n",
    "\n",
    "  return subscription_map\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  subscription_map = generate_clean_data()\n",
    "  compute_lf(subscription_map)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python labeling.py > labeled_churn.csv &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have labeled data, we can split them to have training and test data set. Its better if we check and make sure that our labeling is consistent with the labeled data provided by KKBOX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Churn Dataset Provided by KKBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transaction merge...\n",
      "user logs merge...\n",
      "members merge...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../Data/train.csv')\n",
    "test = pd.read_csv('../Data/sample_submission_zero.csv')\n",
    "\n",
    "transactions = pd.read_csv('../Data/transactions.csv', usecols=['msno'])\n",
    "#transactions = pd.concat((transactions, pd.read_csv('transactions_v2.csv', usecols=['msno'])), axis=0, ignore_index=True).reset_index(drop=True)\n",
    "transactions = pd.DataFrame(transactions['msno'].value_counts().reset_index())\n",
    "transactions.columns = ['msno','trans_count']\n",
    "train = pd.merge(train, transactions, how='left', on='msno')\n",
    "test = pd.merge(test, transactions, how='left', on='msno')\n",
    "transactions = []; print('transaction merge...')\n",
    "\n",
    "#user_logs = pd.read_csv('../input/user_logs_v2.csv', usecols=['msno'])\n",
    "user_logs = pd.read_csv('../Data/user_logs.csv', usecols=['msno'])\n",
    "#user_logs = pd.concat((user_logs, pd.read_csv('../input/user_logs_v2.csv', usecols=['msno'])), axis=0, ignore_index=True).reset_index(drop=True)\n",
    "user_logs = pd.DataFrame(user_logs['msno'].value_counts().reset_index())\n",
    "user_logs.columns = ['msno','logs_count']\n",
    "train = pd.merge(train, user_logs, how='left', on='msno')\n",
    "test = pd.merge(test, user_logs, how='left', on='msno')\n",
    "user_logs = []; print('user logs merge...')\n",
    "\n",
    "members = pd.read_csv('../Data/members_v3.csv')\n",
    "train = pd.merge(train, members, how='left', on='msno')\n",
    "test = pd.merge(test, members, how='left', on='msno')\n",
    "members = []; print('members merge...') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = {'male':1, 'female':2}\n",
    "train['gender'] = train['gender'].map(gender)\n",
    "test['gender'] = test['gender'].map(gender)\n",
    "\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('../Data/transactions.csv') #pd.read_csv('../input/transactions.csv')\n",
    "#transactions = pd.concat((transactions, pd.read_csv('../input/transactions_v2.csv')), axis=0, ignore_index=True).reset_index(drop=True)\n",
    "transactions = transactions.sort_values(by=['transaction_date'], ascending=[False]).reset_index(drop=True)\n",
    "transactions = transactions.drop_duplicates(subset=['msno'], keep='first')\n",
    "\n",
    "train = pd.merge(train, transactions, how='left', on='msno')\n",
    "test = pd.merge(test, transactions, how='left', on='msno')\n",
    "transactions=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.sort_values(by=['date'], ascending=[False])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['msno'], keep='first')\n",
    "    return df\n",
    "\n",
    "def transform_df2(df):\n",
    "    df = df.sort_values(by=['date'], ascending=[False])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['msno'], keep='first')\n",
    "    return df\n",
    "\n",
    "df_iter = pd.read_csv('../Data/user_logs.csv', low_memory=False, iterator=True, chunksize=10000000)\n",
    "last_user_logs = []\n",
    "i = 0 #~400 Million Records - starting at the end but remove locally if needed\n",
    "for df in df_iter:\n",
    "    if i>35:\n",
    "        if len(df)>0:\n",
    "            print(df.shape)\n",
    "            p = Pool(cpu_count())\n",
    "            df = p.map(transform_df, np.array_split(df, cpu_count()))   \n",
    "            df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "            df = transform_df2(df)\n",
    "            p.close(); p.join()\n",
    "            last_user_logs.append(df)\n",
    "            print('...', df.shape)\n",
    "            df = []\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_user_logs.append(transform_df(pd.read_csv('../Data/user_logs_v2.csv')))\n",
    "last_user_logs = pd.concat(last_user_logs, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "last_user_logs = transform_df2(last_user_logs)\n",
    "\n",
    "train = pd.merge(train, last_user_logs, how='left', on='msno')\n",
    "test = pd.merge(test, last_user_logs, how='left', on='msno')\n",
    "last_user_logs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"clean_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"clean_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets check and see if we can only read 2 columns of logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno,date,num_25,num_50,num_75,num_985,num_100,num_unq,total_secs\r\n",
      "rxIP2f2aN0rYNp+toI0Obt/N/FYQX8hcO1fTmmy2h34=,20150513,0,0,0,0,1,1,280.3350\r\n",
      "rxIP2f2aN0rYNp+toI0Obt/N/FYQX8hcO1fTmmy2h34=,20150709,9,1,0,0,7,11,1658.9480\r\n",
      "yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=,20150105,3,3,0,0,68,36,17364.9560\r\n",
      "yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=,20150306,1,0,1,1,97,27,24667.3170\r\n",
      "yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=,20150501,3,0,0,0,38,38,9649.0290\r\n",
      "yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=,20150702,4,0,1,1,33,10,10021.5200\r\n",
      "yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=,20150830,3,1,0,0,4,7,1119.5550\r\n",
      "yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=,20151107,1,0,0,0,4,5,938.0220\r\n",
      "yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=,20160110,2,0,1,0,11,6,3004.0680\r\n"
     ]
    }
   ],
   "source": [
    "#!head ../Data/user_logs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##user_logs = pd.read_csv('../Data/user_logs.csv', usecols=['msno'])\n",
    "#user_logs = pd.concat((user_logs, pd.read_csv('../input/user_logs_v2.csv', usecols=['msno'])), axis=0, ignore_index=True).reset_index(drop=True)\n",
    "#user_logs = pd.DataFrame(user_logs['msno'].value_counts().reset_index())\n",
    "#user_logs.columns = ['msno','logs_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlogs_count = pd.DataFrame(user_logs['msno'].value_counts().reset_index())\\nlogs_count.columns = ['msno','logs_count']\\nuser_logs_25 = user_logs.groupby('msno')['num_25'].statistics.median()\\nuser_logs_25.columns = ['msno','med_25']\\nuser_logs = []\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "logs_count = pd.DataFrame(user_logs['msno'].value_counts().reset_index())\n",
    "logs_count.columns = ['msno','logs_count']\n",
    "user_logs_25 = user_logs.groupby('msno')['num_25'].statistics.median()\n",
    "user_logs_25.columns = ['msno','med_25']\n",
    "user_logs = []\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
